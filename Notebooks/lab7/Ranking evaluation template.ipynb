{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking evaluation metrics\n",
    "\n",
    "In this lab you will implement ranking evaluation metrics.\n",
    "\n",
    "For each query the search engine returns the sorted list of documents. We expect to have relevant documents at the top. \n",
    "In supervision learning to evaluate the quality we need labeled data.\n",
    "\n",
    "Read this article first:\n",
    "http://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples\n",
    "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance metrics\n",
    "\n",
    "We will start from the assumption, that each document is either relevant or not.\n",
    "\n",
    "#### recall, precision, f1\n",
    "\n",
    "First metrics are already faminiar for you from Introduction to Machine Learning course.\n",
    "Implement relall, precision, and relevance for top $k$ documents.\n",
    "\n",
    "relevance is a list which represents that document with this index in ranking is relevant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def recall_at_k(relevance, k):\n",
    "    return sum(relevance[:k]) / (sum(relevance[:k]) + sum(relevance[k:]))\n",
    "\n",
    "def precision_at_k(relevance, k):\n",
    "    return sum(relevance[:k])/k\n",
    "\n",
    "def f1_at_k(relevance, k):\n",
    "    r, p = precision_at_k(relevance, k), recall_at_k(relevance, k)\n",
    "    return 2 * r * p / (r + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "1.0\n",
      "1.0\n",
      "0.5714285714285714\n",
      "0.4\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "r = [1, 0, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "print(recall_at_k(r, 1))\n",
    "print(recall_at_k(r, 8))\n",
    "\n",
    "print(precision_at_k(r, 1))\n",
    "print(precision_at_k(r, 7))\n",
    "\n",
    "print(f1_at_k(r, 1))\n",
    "print(f1_at_k(r, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Precision\n",
    "You can calculate the AP using the following algorithm:\n",
    "\n",
    "<img src=\"http://queirozf.com/images/contents/mnc7sx1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevance, k):\n",
    "    running_sum = np.array(np.cumsum(relevance[:k])) * np.array(relevance[:k])\n",
    "    running_sum = running_sum / np.array([i for i in range(1,k+1,1)])\n",
    "    \n",
    "    return sum(running_sum) / sum(relevance[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: 1.0\n",
      "Rank 2: 1.0\n",
      "Rank 3: 0.8333333333333333\n",
      "Rank 4: 0.8055555555555555\n",
      "Rank 5: 0.8055555555555555\n",
      "Rank 6: 0.7708333333333333\n",
      "Rank 7: 0.7708333333333333\n",
      "Rank 8: 0.7708333333333333\n"
     ]
    }
   ],
   "source": [
    "print(*[\"Rank \" + str(i)+ \": \" + str(average_precision(r, i)) for i in range(1,9)], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance as a real number\n",
    "\n",
    "#### DCG\n",
    "\n",
    "DCG - discounted cumulative gain, does't require the relevance to be a binary feature. In many situations one document is more relevant than another and we want to represent it in supervised evaluation. Often the relevance is a number form ${0,1,2,3}$, but ${0,1}$ is also appropriate for usage.\n",
    "\n",
    "The idea is that each rerelvant document brings a \"gain\" for a user. He or she looks through the documents from the first. So the gain sums cumulatively. But it is better to have a relevant document at the top of the ranking, so the weight of that gain decreases, or we have a discounded weight with increasing of the document position. And since the weight is decreasing, we can calculate this value for the top $k$ documents in ranking.\n",
    "\n",
    "$$DCG@k = \\sum_{1}^{k}\\frac{2^{rel_i}-1}{log_2(i+1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance, k=10):\n",
    "    relevance = np.array(relevance)\n",
    "    #relevance = relevance / np.sqrt(np.dot(relevance, relevance))\n",
    "    return sum(map(lambda rel, i: (2 ** rel - 1) / math.log2(i+2), \n",
    "                   relevance[:k], [i for i in range(k)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.5\n",
      "2.2868837451814152\n",
      "7.0\n",
      "16.80260104782745\n"
     ]
    }
   ],
   "source": [
    "r2 = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "\n",
    "\n",
    "print(dcg_at_k(r, 1))\n",
    "print(dcg_at_k(r, 3))\n",
    "print(dcg_at_k(r, 8))\n",
    "\n",
    "print(dcg_at_k(r2, 1))\n",
    "print(dcg_at_k(r2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nDCG\n",
    "Now the idea is to normalize it to the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(relevance, k=10):\n",
    "    sorted_relevance = list(reversed(sorted(relevance)))\n",
    "    return dcg_at_k(relevance, k) / dcg_at_k(sorted_relevance,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: 1.0\n",
      "Rank 2: 0.6131471927654584\n",
      "Rank 3: 0.7039180890341347\n",
      "Rank 4: 0.75369761125927\n",
      "Rank 5: 0.75369761125927\n",
      "Rank 6: 0.8927537907700456\n",
      "Rank 7: 0.8927537907700456\n",
      "Rank 8: 0.8927537907700456\n"
     ]
    }
   ],
   "source": [
    "print(*[\"Rank \" + str(i)+ \": \" + str(ndcg_at_k(r, i)) for i in range(1,len(r) + 1)], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test in real data\n",
    "\n",
    "You already have search engines on songs or news. Test the evaluation on real data.\n",
    "\n",
    "1. Choose the search query query\n",
    "2. Run the search\n",
    "3. Manually look top 10 results and evaluate each of them if it relevant or not\n",
    "4. Calculate AP and DCG (relevance is either 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"food aid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
