{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving vector-space model\n",
    "\n",
    "This lab will be devoted to the use of `doc2vec` model for the needs of information retrieval and text classification.  \n",
    "\n",
    "## 1. Searching in the curious facts database\n",
    "The facts dataset is given [here](https://github.com/hsu-ai-course/hsu.ai/blob/master/code/datasets/nlp/facts.txt), take a look.  We want you to retrieve facts relevant to the query, for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using `doc2vec` model.\n",
    "\n",
    "### 1.1 Loading trained `doc2vec` model\n",
    "\n",
    "First, let's load the pre-trained `doc2vec` model from https://github.com/jhlau/doc2vec (Associated Press News DBOW (0.6GB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.doc2vec.Doc2Vec'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# unpack a model into 3 files and target the main one\n",
    "# doc2vec.bin  <---------- this\n",
    "# doc2vec.bin.syn0.npy\n",
    "# doc2vec.bin.sin1neg.npy\n",
    "model = Doc2Vec.load('doc2vec.bin', mmap=None)\n",
    "print(type(model))\n",
    "print(type(model.infer_vector([\"to\", \"be\", \"or\", \"not\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading data\n",
    "\n",
    "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO read facts into list\n",
    "facts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n",
      "2. McDonalds calls frequent buyers of their food “heavy users.”\n",
      "3. The average person spends 6 months of their lifetime waiting on a red light to turn green.\n",
      "4. The largest recorded snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
      "5. You burn more calories sleeping than you do watching television.\n"
     ]
    }
   ],
   "source": [
    "print(*facts[:5], sep='\\n')\n",
    "\n",
    "assert len(facts) == 159\n",
    "assert ('our lovely little planet') in facts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Transforming sentences to vectors\n",
    "\n",
    "Transform the list of facts to numpy array of vectors corresponding to each document (`sent_vecs`), inferring them from the model we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO infer vectors\n",
    "sent_vecs = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 300)\n"
     ]
    }
   ],
   "source": [
    "print(sent_vecs.shape)\n",
    "assert sent_vecs.shape == (159, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Find closest\n",
    "\n",
    "Now, reusing the code from the last lab, find facts which are closest to the query using cosine similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: good mood\n",
      "\t 68. Cherophobia is the fear of fun. sim= 0.5935387\n",
      "\t 76. You breathe on average about 8,409,600 times a year sim= 0.5562612\n",
      "\t 144. Dolphins sleep with one eye open! sim= 0.5465349\n",
      "\t 97. 111,111,111 X 111,111,111 = 12,345,678,987,654,321 sim= 0.54036695\n",
      "\t 18. You cannot snore and dream at the same time. sim= 0.5353584\n"
     ]
    }
   ],
   "source": [
    "#TODO output closest facts to the query\n",
    "query = \"good mood\"\n",
    "\n",
    "print(\"Results for query:\", query)\n",
    "for k, v, p in r:\n",
    "    print(\"\\t\", facts[k], \"sim=\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training doc2vec model and documents classifier\n",
    "\n",
    "Now we would like you to train doc2vec model yourself based on [this topic-modeling dataset](https://code.google.com/archive/p/topic-modeling-tool/downloads).\n",
    "\n",
    "### 2.1 Read dataset\n",
    "\n",
    "First, read the dataset - it consists of 4 parts, you need to merge them into single list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO read the dataset into list\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15002\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data))\n",
    "assert len(all_data) == 15002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training `doc2vec` model\n",
    "\n",
    "Train a `doc2vec` model based on the dataset you've loaded. The example of training is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']] \n",
      "\n",
      "[TaggedDocument(words=['human', 'interface', 'computer'], tags=[0]), TaggedDocument(words=['survey', 'user', 'computer', 'system', 'response', 'time'], tags=[1]), TaggedDocument(words=['eps', 'user', 'interface', 'system'], tags=[2]), TaggedDocument(words=['system', 'human', 'system', 'eps'], tags=[3]), TaggedDocument(words=['user', 'response', 'time'], tags=[4]), TaggedDocument(words=['trees'], tags=[5]), TaggedDocument(words=['graph', 'trees'], tags=[6]), TaggedDocument(words=['graph', 'minors', 'trees'], tags=[7]), TaggedDocument(words=['graph', 'minors', 'survey'], tags=[8])] \n",
      "\n",
      "[ 0.02381649 -0.02886504  0.02321507  0.0241891  -0.02796458]\n"
     ]
    }
   ],
   "source": [
    "#TODO change this according to the task\n",
    "# small set of tokenized sentences\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# just a test set of tokenized sentences\n",
    "print(common_texts, \"\\n\")\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "print(documents, \"\\n\")\n",
    "# train a model\n",
    "model = Doc2Vec(\n",
    "    documents,     # collection of texts\n",
    "    vector_size=5, # output vector size\n",
    "    window=2,      # maximum distance between the target word and its neighboring word\n",
    "    min_count=1,   # minimal number of \n",
    "    workers=4      # in parallel\n",
    ")\n",
    "\n",
    "# clean training data\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# save and load\n",
    "model.save(\"d2v.model\")\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "vec = model.infer_vector([\"system\", \"response\"])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Form train and test datasets\n",
    "\n",
    "Transform documents to vectors and split data to train and test sets. Make sure that the split is stratified as the classes are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO transforn and make a train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Train topics classifier\n",
    "\n",
    "Train a classifier that would classify any document to one of four categories: fuel, brain injury, music, and economy.\n",
    "Print a classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO train a classifier and measure its performance\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which class is the hardest one to recognize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Bonus task\n",
    "\n",
    "What if we trained our `doc2vec` model using window size = 5 or 10? Would it improve the classification acccuracy? What about vector dimensionality? Does it mean that increasing it we will achieve better performance in terms of classification?\n",
    "\n",
    "Explore the influence of these parameters on classification performance, visualizing it as a graph (e.g. window size vs f1-score, vector dim vs f1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO bonus task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
