{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-NaMyiXzPBi"
   },
   "source": [
    "# 1. Text processing\n",
    "\n",
    "We will create the pipline of text preprocessing\n",
    "\n",
    "# 1. 1 Normalization\n",
    "\n",
    "The first step is normalisation.\n",
    "It might include:\n",
    "* converting all letters to lower or upper case\n",
    "* converting numbers into words or removing numbers\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing white spaces\n",
    "* expanding abbreviations\n",
    "\n",
    "In this exercise it would be ok to have a lowercase text without specific characters and digits and without unnecessery space symbols.\n",
    "\n",
    "How neural networks could be implemented for text normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in /home/jafar/anaconda3/lib/python3.7/site-packages (1.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "JioPN9wvn4-R",
    "outputId": "c6eb6ab7-8e78-4bcb-99a7-54248e880dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: config in /home/jafar/anaconda3/lib/python3.7/site-packages (0.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9C6sjLcAn4-7",
    "outputId": "4134c0a4-1e3f-4535-ad80-6f62e39cbf4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/jafar/anaconda3/lib/python3.7/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jafar/anaconda3/lib/python3.7/site-packages (from bs4) (4.8.0)\r\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/jafar/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4) (1.9.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-2VsHEozn4_T",
    "outputId": "66ba4355-4c74-4ab0-de9e-fc42b6d0b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/jafar/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/jafar/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘misc’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fKTNvYc9dMR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "import json\n",
    "# normilize text\n",
    "def normalize(text, is_query=False):\n",
    "    text = unidecode.unidecode(text) # remove accents\n",
    "    text = re.sub('(\\w)',lambda m: m.group(0).lower(),text) # to_lower the entire text\n",
    "    if is_query:\n",
    "        text = re.sub('[^a-z $ *]', \"\", text) # remove punctuations\n",
    "    else:\n",
    "        text = re.sub('[^a-z ]', \"\", text)\n",
    "    text = re.sub('(\\ +)', \" \", text) # if we have more than one white space it will become one\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KECQdAEf_hFs",
    "outputId": "d5089af0-093d-45c9-a8e2-c7a859ba1aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "borrowed from latins teachers drunks niggas per he drank he killed hell se by itself from per by through and se itself himself herself themselves\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Borrowed from \\n\\n Latins* $ teachers drunks niggas per he drank, he killed  he'll  \\\"\\\"\\\"\\\"\\'\\'   sē (“by itself”), from per (“by, through”) and sē (“itself, himself, herself, themselves”)\"\"\"\n",
    "\n",
    "text = normalize(text, False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnF7atXqCHfu"
   },
   "source": [
    "# 1.2 Tokenize\n",
    "Use nltk tokenizer to tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WN2ISxpCCzc6"
   },
   "outputs": [],
   "source": [
    "# tokenize text using nltk lib\n",
    "import nltk\n",
    "import config \n",
    "config.flag = False\n",
    "def download_packages():\n",
    "    if not config.flag:\n",
    "        print(nltk.download('punkt'))\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "    config.flag = True\n",
    "def tokenize(text):\n",
    "    \n",
    "    download_packages()\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "eOmr8_unC16S",
    "outputId": "5f7212fc-dac1-4bb5-bc93-2b54102f7deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['borrowed', 'from', 'latins', 'teachers', 'drunks', 'niggas', 'per', 'he', 'drank', 'he', 'killed', 'hell', 'se', 'by', 'itself', 'from', 'per', 'by', 'through', 'and', 'se', 'itself', 'himself', 'herself', 'themselves']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jafar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jafar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jafar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SBPFuytDKmx"
   },
   "source": [
    "# 1.3 Lemmatization\n",
    "What is the difference between stemming and lemmatization?\n",
    "\n",
    "[Optional reading](https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "def lemmatization(tokens):\n",
    "    lm = WordNetLemmatizer()\n",
    "    return list(map(lm.lemmatize,tokens))\n",
    "\n",
    "def stemmatiztion(tokens):\n",
    "    ps = SnowballStemmer(language='english')\n",
    "    return list(map(ps.stem,tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zPz08eJbDbYn",
    "outputId": "0ac3d3c7-98c6-40a5-b19c-d92d0c871547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borrowed', 'from', 'latin', 'teacher', 'drunk', 'nigga', 'per', 'he', 'drank', 'he', 'killed', 'hell', 'se', 'by', 'itself', 'from', 'per', 'by', 'through', 'and', 'se', 'itself', 'himself', 'herself', 'themselves']\n"
     ]
    }
   ],
   "source": [
    "lemmed = lemmatization(tokens)\n",
    "print(lemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AE7QW2Akn5CK",
    "outputId": "9374f648-800f-4f14-d704-275d318be931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borrow', 'from', 'latin', 'teacher', 'drunk', 'nigga', 'per', 'he', 'drank', 'he', 'kill', 'hell', 'se', 'by', 'itself', 'from', 'per', 'by', 'through', 'and', 'se', 'itself', 'himself', 'herself', 'themselv']\n"
     ]
    }
   ],
   "source": [
    "stemmed = stemmatiztion(tokens)\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOPWqChkD-Rl"
   },
   "source": [
    "# 1.4 Stop words\n",
    "The next step is to remove stop words. Take the list of stop words from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo7vs0SvEHdi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_stop_word(tokens):\n",
    "    stopping_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stopping_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7ifHBaaIEpRo",
    "outputId": "55e6fcb3-4b86-428d-93e7-eb7bea378aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borrow', 'latin', 'teacher', 'drunk', 'nigga', 'per', 'drank', 'kill', 'hell', 'se', 'per', 'se', 'themselv']\n"
     ]
    }
   ],
   "source": [
    "clean = remove_stop_word(stemmed)\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AK2rqM0UEocc"
   },
   "source": [
    "# 1.5 Pipeline\n",
    "Run a complete pipeline inone function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsXF8DD6xWj2"
   },
   "outputs": [],
   "source": [
    "def preprocess(text, is_query=False):\n",
    "    # TODO\n",
    "    res = remove_stop_word(lemmatization(tokenize(normalize(text,is_query))))\n",
    "    if not is_query:\n",
    "        return [\"$\"+i+\"$\" for i in res]\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zsAlLaBuFZIJ",
    "outputId": "6287ed94-8d3e-4ae2-dd3d-ccdc7c793830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$borrowed$', '$latin$', '$teacher$', '$drunk$', '$nigga$', '$per$', '$drank$', '$killed$', '$hell$', '$se$', '$per$', '$se$']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clean = preprocess(text)\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqM29ujB6Y9x"
   },
   "source": [
    "# 2. Collection\n",
    "\n",
    "Download Reuters data from here:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz\n",
    "\n",
    "Read data description here:\n",
    "https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection\n",
    "\n",
    "The function should return a list of strings - raw texts. Remove html tags using bs4 package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VxxzUFKTfXg"
   },
   "source": [
    "## 2.1 Alternative (0.5 task bonus points)\n",
    "\n",
    "Download songs (the process takes time, 1000 documents might be enough for a sake of exercise) from https://www.lyrics.com/. Implement a text search on it. In this case you have to creare class *Song* with fiels *title*, *artist* *and* text. The collection will contain a list of songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Save collection.json\n",
    "\n",
    "Here just retrieve the json if it exists to save time while testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwO9YHvV6cXZ"
   },
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "import os.path\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "\n",
    "def download(url,filepath):\n",
    "    if os.path.isfile(filepath):\n",
    "        return 0\n",
    "    else:\n",
    "        req.urlretrieve(url,filepath)\n",
    "        \n",
    "def get_collection():\n",
    "    if os.path.isfile('collection.json'):\n",
    "        with open('collection.json','r') as fd:\n",
    "            return json.load(fd)      \n",
    "    download('https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz','./misc/reuters.tar.gz')\n",
    "    tf = tarfile.open(\"./misc/reuters.tar.gz\")\n",
    "    tf.extractall('./misc')\n",
    "    collection = []\n",
    "    names = []\n",
    "    counter = 0\n",
    "    pattern = re.compile(\".+\\.sgm$\")\n",
    "    for file in os.listdir(\"./misc\"):\n",
    "        file = './misc/' + file\n",
    "        if not pattern.match(file):\n",
    "            continue\n",
    "        collection.append([])    \n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                soup = BeautifulSoup(line)\n",
    "                collection[-1].extend(preprocess(soup.get_text()))\n",
    "        \n",
    "        print(file)\n",
    "        names.append(counter)\n",
    "        counter+=1\n",
    "#         if counter == 3:\n",
    "#             break\n",
    "    \n",
    "    return [(collection[i],names[i]) for i in range(len(collection))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiunOmRMIT47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "collection = get_collection()\n",
    "\n",
    "print(len(collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Save collection\n",
    "\n",
    "save collection for fast testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "_Zctycx6TOJS",
    "outputId": "2187d988-3108-4f43-f8bd-c4d143bf2078"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('collection.json', 'w') as fd:\n",
    "    json.dump(collection, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Wild card processing and inserting in the data structure\n",
    "\n",
    "For example having words like 'word' it will be transformed into a collection of bigrams \n",
    "to [$\"\\$w\",\"wo\",\"or\",\"rd\",\"d\\$$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_n_gram(tokens, n=2): # default is a bigram\n",
    "    if type(tokens) != type([]):\n",
    "        #print(type(tokens))\n",
    "        tokens = [tokens]\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        \n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        res += [\"\".join(ngram) for ngram in ngrams]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$h', 'he', 'el', 'll', 'lo', 'o$', '$j', 'ja']\n",
      "['$h', 'he', 'el', 'll', 'lo', 'o$']\n"
     ]
    }
   ],
   "source": [
    "print(to_n_gram(['$hello$','$ja'],2))\n",
    "print(to_n_gram('$hello$',2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Word Index\n",
    "\n",
    "Here we will have the index that matches the word to the respected n gram\n",
    "\n",
    "## get_cached, write cached\n",
    "\n",
    "these two functions are not a part of the assignment and I used them to make testing on my machines faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    " \n",
    "def get_cached():\n",
    "    a,b = {}, {}\n",
    "    if os.path.isfile('inverted_word_index.json'):\n",
    "        with open('inverted_word_index.json','r') as fd:\n",
    "            a = json.load(fd)\n",
    "            a = {vi:set(a[vi]) for vi in a.keys()}\n",
    "    if os.path.isfile('ngram_word_index.json'):\n",
    "        with open('ngram_word_index.json','r') as fd:\n",
    "            b = json.load(fd)\n",
    "            b = {vi:set(b[vi]) for vi in b.keys()}\n",
    "    return a,b\n",
    "\n",
    "def write_cached(inverted_word_index, ngram_word_index):\n",
    "    a,b = inverted_word_index, ngram_word_index\n",
    "    print(type(a) , type(b))\n",
    "    with open('inverted_word_index.json','w') as fd:\n",
    "\n",
    "        data = {vi:list(a[vi]) for vi in a.keys()}\n",
    "        a = json.dump(data, fd)\n",
    "    with open('ngram_word_index.json','w') as fd:\n",
    "        data = {vi:list(b[vi]) for vi in b.keys()}\n",
    "        json.dump(data, fd)\n",
    "    return 'Success'\n",
    "    \n",
    "def make_word_index(collection, force= False):\n",
    "    inverted_word_index = {}\n",
    "    ngram_word_index = {}\n",
    "    if not force:\n",
    "        a,b = get_cached()\n",
    "        if bool(a) and bool(b):\n",
    "            return a,b\n",
    "    print('gg')\n",
    "    for (group, name) in collection:\n",
    "        for word in group:\n",
    "            if word in inverted_word_index.keys():\n",
    "                continue\n",
    "            else:\n",
    "                ngrams = to_n_gram(word)\n",
    "                inverted_word_index[word] = ngrams\n",
    "                for gram in set(ngrams):\n",
    "                    \n",
    "                    d = ngram_word_index.setdefault(gram,set([]))\n",
    "                    d.add(word)\n",
    "    write_cached(inverted_word_index, ngram_word_index)\n",
    "    return inverted_word_index, ngram_word_index\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_word_index, ngram_word_index = make_word_index(collection)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Wildcard queries\n",
    "\n",
    "here I will try to reclaim all the words that match a wild card query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def wild_find(token, ngram_word_index):\n",
    "    temp_token = token\n",
    "    if len(token) < 2:\n",
    "        return \"Minimum length should be 2 letters\"\n",
    "    if token[-1] == '*':\n",
    "        token = token[:-1]\n",
    "        token = \"$\"+token\n",
    "    elif token[0] == '*':\n",
    "        token = token[1:] + \"$\"\n",
    "    elif token.find(\"*\")!=-1:\n",
    "        token = token.split('*')\n",
    "        token[0] = \"$\" + token[0]\n",
    "        token[1] = token[1] + \"$\"\n",
    "   \n",
    "    ngrams = to_n_gram(token)\n",
    "    #print(ngrams)\n",
    "    A = ngram_word_index[ngrams[0]]\n",
    "    temp_token = \"\\$\"+temp_token.replace(\"*\",\"[a-z]*\")+\"\\$\"\n",
    "    #print(temp_token)\n",
    "    pattern = re.compile(temp_token)\n",
    "   \n",
    "    answers =  A.intersection(*[ngram_word_index[vi] for vi in ngrams[1:]])\n",
    "    return [i for i in answers if pattern.match(i)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for 'h*ell' are \n",
      " ['$howell$', '$hartnell$', '$honeywell$', '$hell$', '$hopewell$']\n",
      "results for 'j*far' are \n",
      " ['$jaafar$']\n",
      "results for 'jaafar' are \n",
      " ['$jaafar$']\n",
      "['$klms$', '$kimal$', '$kistler$', '$keatings$', '$kiyonga$', '$kota$', '$kirk$', '$kamloops$', '$kindled$', '$kellwood$', '$keewatin$', '$krenzler$', '$khoo$', '$keta$', '$khon$', '$kompagni$', '$knapp$', '$kcsi$', '$krishna$', '$krupp$', '$kitty$', '$known$', '$kosan$', '$ko$', '$kiena$', '$kotch$', '$kaufhof$', '$kertosastro$', '$khashoggi$', '$kuwaitussr$', '$keswick$', '$kwextv$', '$kilowatthours$', '$kenai$', '$kurzkasch$', '$kurdish$', '$kims$', '$kitwe$', '$kampala$', '$krumper$', '$kickoff$', '$kittiwake$', '$ka$', '$kyushu$', '$knowledgable$', '$kohldelors$', '$kuroda$', '$kinzoku$', '$keenly$', '$kennametal$', '$karda$', '$kerkorian$', '$kmg$', '$kimmelman$', '$kamel$', '$kp$', '$kleckner$', '$klaus$', '$kk$', '$kauai$', '$knockdown$', '$kingsleyjones$', '$kleve$', '$keatinghawke$', '$kepco$', '$krolik$', '$karachi$', '$krugerrands$', '$katrine$', '$keflex$', '$kravis$', '$kenyaukwestgermany$', '$khamenei$', '$kreditkasse$', '$kamikaze$', '$kero$', '$kind$', '$kitchen$', '$katsuhiko$', '$kirya$', '$kuntze$', '$kaypro$', '$kiichi$', '$kl$', '$kastenmeier$', '$kerir$', '$kuwaitiraniraqbahrainkuwaitomanqatarsaudiarabiauae$', '$knight$', '$kunio$', '$kentmoore$', '$kingwood$', '$kiddie$', '$kimberley$', '$kombo$', '$kinnear$', '$keikichi$', '$keltic$', '$kenca$', '$kairey$', '$keiichi$', '$klootwijk$', '$kollmorgen$', '$kline$', '$khomeini$', '$krugman$', '$kagaku$', '$knoll$', '$kredietbank$', '$kaen$', '$kcbts$', '$kesselwerke$', '$karolinska$', '$kellner$', '$keng$', '$kelley$', '$krelitz$', '$kenan$', '$kennecott$', '$knell$', '$kuala$', '$kawgf$', '$kta$', '$kurram$', '$kumagai$', '$kilobyte$', '$kenilworth$', '$kaplan$', '$kren$', '$kai$', '$knock$', '$kr$', '$knightridder$', '$kdono$', '$kitchener$', '$krumm$', '$kentucky$', '$karnosky$', '$kyoto$', '$keiji$', '$kerna$', '$kml$', '$kflto$', '$korthals$', '$korhonen$', '$khd$', '$knotty$', '$ky$', '$klauhs$', '$ktxadallas$', '$koichi$', '$kardamanend$', '$kwik$', '$kurt$', '$kmsi$', '$kwus$', '$kaya$', '$kanasa$', '$kurz$', '$knmc$', '$karnatake$', '$koei$', '$kobeya$', '$katz$', '$kawade$', '$karmila$', '$kentuckybased$', '$karncharoendi$', '$kaspar$', '$kimura$', '$khark$', '$koninklijke$', '$khelaf$', '$kierulff$', '$kjobenhavn$', '$kuznetsky$', '$kamentsev$', '$kuoshu$', '$kissinger$', '$keidanren$', '$kaczura$', '$koji$', '$keeled$', '$kyriakides$', '$koreantaiwan$', '$kampelman$', '$kuranari$', '$kaolin$', '$khi$', '$korth$', '$kuwait$', '$kremlinwatchers$', '$koziol$', '$kenneth$', '$kelso$', '$kproo$', '$king$', '$ku$', '$keh$', '$kardemand$', '$kreditanstalt$', '$kent$', '$khalid$', '$koaam$', '$kouame$', '$krapels$', '$kanis$', '$kilobit$', '$kid$', '$kansallisosakepankki$', '$knuettel$', '$kinney$', '$kidnapping$', '$kraft$', '$knowlegeable$', '$karin$', '$kusumaatmadja$', '$kltj$', '$klynveld$', '$kt$', '$killen$', '$kongbased$', '$kyoji$', '$kapsis$', '$kemp$', '$kelvin$', '$koppers$', '$kia$', '$kduwi$', '$km$', '$kaduna$', '$kimbo$', '$kwp$', '$kettle$', '$kara$', '$kantyka$', '$kkk$', '$kokusan$', '$kcsttv$', '$kumura$', '$kerdix$', '$kin$', '$krall$', '$kyowas$', '$karlsruhe$', '$kelton$', '$khoos$', '$kaakebeen$', '$knox$', '$knssfm$', '$kantonalbank$', '$knudsen$', '$kri$', '$kendall$', '$killer$', '$krogers$', '$kill$', '$khjtv$', '$kika$', '$khic$', '$keynote$', '$kemper$', '$kommerzialbank$', '$keller$', '$kbh$', '$kec$', '$khadeir$', '$kilimanjaro$', '$kogaku$', '$ketu$', '$kirksville$', '$karnes$', '$ktii$', '$kyle$', '$koerner$', '$kickback$', '$krefeld$', '$kuwaitusa$', '$kian$', '$kheng$', '$kapparadiation$', '$kly$', '$kean$', '$kawerau$', '$kvlm$', '$kaputin$', '$kuwaitsaudiarabiaegyptmoroccoiraqqatarbahrain$', '$kapchorwa$', '$keystone$', '$kenro$', '$kynge$', '$koch$', '$kazakhstan$', '$korths$', '$koh$', '$ksu$', '$kilometre$', '$kobena$', '$knickerbocker$', '$korea$', '$kearny$', '$karasmasoglu$', '$ktel$', '$kingsbridge$', '$kevx$', '$kroger$', '$kincaid$', '$kee$', '$koido$', '$kommanditgesellschaft$', '$kiamse$', '$kiechle$', '$kapok$', '$kina$', '$kendallville$', '$kneedeep$', '$kotler$', '$kick$', '$kokan$', '$kalevi$', '$ktron$', '$kleinworts$', '$kerridge$', '$kilowatt$', '$klopfenstein$', '$koppabergs$', '$kobe$', '$kilolitre$', '$kaufel$', '$kemc$', '$kembla$', '$keith$', '$klagenfurt$', '$kplmsi$', '$kazuyuki$', '$kof$', '$komtt$', '$khartoum$', '$kassenobligation$', '$kato$', '$klaipeda$', '$kohlberg$', '$kamine$', '$kenyan$', '$koren$', '$kassebaum$', '$khartoumbased$', '$khaled$', '$kirkorian$', '$kandeel$', '$kidnapper$', '$karani$', '$knowingly$', '$kaohsiung$', '$knowhow$', '$kootenay$', '$kasuhiza$', '$kwd$', '$kelto$', '$knuckle$', '$keersmaeker$', '$kanan$', '$kulpsville$', '$kamga$', '$kohlciampistoltenbergpoehl$', '$klma$', '$kelman$', '$koepfgen$', '$kashijima$', '$kalamazoo$', '$kosters$', '$kmslong$', '$kaneb$', '$krieger$', '$kohlchiracmitterrand$', '$kawst$', '$khur$', '$katy$', '$kebo$', '$kennedy$', '$kippur$', '$knowbut$', '$kawht$', '$knocking$', '$kearn$', '$kv$', '$k$', '$kep$', '$knickerbockers$', '$kawakami$', '$kg$', '$kristiansen$', '$ktas$', '$kim$', '$kviool$', '$krlz$', '$khalaf$', '$kgaa$', '$kosovo$', '$klesch$', '$kjell$', '$killington$', '$kna$', '$khalil$', '$kwh$', '$kirtland$', '$kasl$', '$kfw$', '$kumakiri$', '$keeping$', '$kidnapped$', '$kblll$', '$kondo$', '$keycorp$', '$krugerrand$', '$kotc$', '$kerry$', '$korczag$', '$kahle$', '$kloeckners$', '$kit$', '$kimball$', '$kftv$', '$konsgberg$', '$klu$', '$keyserv$', '$kos$', '$kampuchea$', '$kelly$', '$keen$', '$keating$', '$kmws$', '$kpi$', '$kenzaburo$', '$kono$', '$kenosha$', '$karcher$', '$keb$', '$knowledgeable$', '$karami$', '$keane$', '$keep$', '$krl$', '$kluwer$', '$kuwaitusairan$', '$khalifa$', '$kencao$', '$karim$', '$kinburn$', '$knocked$', '$kpt$', '$kowlooncanton$', '$kilcullen$', '$killlington$', '$kohl$', '$kpmg$', '$kotaddu$', '$koaqfm$', '$kron$', '$kodak$', '$kurm$', '$kronish$', '$koki$', '$knpnas$', '$ktiio$', '$kertih$', '$kepcos$', '$kingston$', '$kwelagobe$', '$kidokoro$', '$kleyn$', '$kankani$', '$kop$', '$khj$', '$knitwear$', '$kiel$', '$kansa$', '$killing$', '$krone$', '$koo$', '$kdny$', '$kearfott$', '$kvil$', '$kredietbanks$', '$kmsio$', '$kenyapakistan$', '$kericho$', '$kawasumi$', '$kmdc$', '$kazuya$', '$knwo$', '$kurdistan$', '$koreabased$', '$kazer$', '$knokkeheist$', '$korkman$', '$koblenz$', '$kokomo$', '$kamplemans$', '$kieves$', '$kuehler$', '$kwinana$', '$kyosuke$', '$koether$', '$khoorelated$', '$krezmar$', '$kerr$', '$klkgf$', '$keppel$', '$ksc$', '$kerno$', '$kahl$', '$kahn$', '$kram$', '$kullberg$', '$koreanmade$', '$kobuse$', '$krpgd$', '$kloeckner$', '$kochan$', '$kuohua$', '$kastl$', '$karczag$', '$kafanchan$', '$kikinzoku$', '$kingstown$', '$knoell$', '$kongmanila$', '$knockon$', '$knobbs$', '$ktco$', '$kakuei$', '$kodiak$', '$katharina$', '$keyed$', '$kakimoto$', '$kenaf$', '$keppels$', '$kharg$', '$kronor$', '$kidston$', '$keefe$', '$knowledge$', '$kolosky$', '$kreisler$', '$kandice$', '$karamis$', '$kan$', '$kuwaiticonnected$', '$koffi$', '$kongsberg$', '$kong$', '$kwinter$', '$kirinyaga$', '$kuypers$', '$kiloliter$', '$kirschner$', '$kraftwerk$', '$kodel$', '$keynes$', '$keigger$', '$kglm$', '$kritchevsky$', '$kresge$', '$keithley$', '$kyowa$', '$kahan$', '$kenyauganda$', '$kgb$', '$kramer$', '$koreausda$', '$kirchhain$', '$krueger$', '$kako$', '$kay$', '$kwok$', '$kuaiti$', '$kidso$', '$knutson$', '$kevin$', '$kharrazi$', '$krpgf$', '$kathleen$', '$kunayev$', '$kilometer$', '$knapps$', '$kitakyushu$', '$kaltenbacher$', '$kazuo$', '$kmwso$', '$kdd$', '$keizai$', '$koos$', '$kahului$', '$keyc$', '$kloecknerwerke$', '$kyat$', '$kicking$', '$kowloon$', '$kidder$', '$klesh$', '$kado$', '$kdi$', '$kali$', '$krpbf$', '$know$', '$kuwaiti$', '$kohlique$', '$koivisto$', '$klt$', '$kmb$', '$kenner$', '$kerrmcgee$', '$kaul$', '$kmw$', '$kappakparadiationrads$', '$kawasaki$', '$kuwaitegypt$', '$kluser$', '$kazuaki$', '$kleinerts$', '$klugt$', '$kasgeldleningen$', '$kcbt$', '$kurihara$', '$konishi$', '$kra$', '$kingsport$', '$kadoorie$', '$kremlin$', '$kuomintang$', '$katigbak$', '$kennan$', '$kuwaitiowned$', '$ketza$', '$konstantin$', '$knife$', '$kingsley$', '$konglisted$', '$kimbark$', '$kji$', '$kilotonnes$', '$kanemaru$', '$klingensmith$', '$kangyos$', '$karasu$', '$karasawa$', '$klmas$', '$kerb$', '$kennebec$', '$kerslake$', '$kathy$', '$kaines$', '$kevex$', '$ketchup$', '$kosaku$', '$kasahara$', '$keita$', '$kamchorn$', '$knot$', '$koethers$', '$knoxville$', '$kappa$', '$kisumu$', '$krung$', '$kuppenheimer$', '$kansai$', '$kenzo$', '$kevan$', '$korat$', '$kyodo$', '$keystroke$', '$kasier$', '$kdsi$', '$konover$', '$kalispell$', '$kingman$', '$kashikojima$', '$killian$', '$kowlhk$', '$kenyon$', '$kopparbergs$', '$kwesi$', '$kslll$', '$kassem$', '$kuwaitecuador$', '$knowlton$', '$kaunda$', '$kogyo$', '$kenosa$', '$karen$', '$kerin$', '$kindness$', '$kashing$', '$kuykenball$', '$kontrollbank$', '$kangyo$', '$kiuchi$', '$kevlin$', '$kndr$', '$kwikl$', '$karlheinz$', '$kubota$', '$klrt$', '$kinabalu$', '$klukwan$', '$kotowski$', '$kenny$', '$kenners$', '$ken$', '$key$', '$kommunalbank$', '$kurzweil$', '$kassel$', '$kligman$', '$kwacha$', '$kilowatthour$', '$koolen$', '$kress$', '$karllorimar$', '$kedar$', '$kwu$', '$koehler$', '$kaysersberg$', '$kock$', '$kingfahd$', '$kredit$', '$kiatpaiboon$', '$ktxhhouston$', '$klowden$', '$kwvt$', '$kilogram$', '$knowing$', '$kuenheim$', '$kaepa$', '$keelerdorroliver$', '$kc$', '$kalo$', '$kuhlmann$', '$kriwet$', '$kurlack$', '$kredi$', '$kacek$', '$krohn$', '$kanon$', '$knorr$', '$kleinwort$', '$kolbin$', '$ksf$', '$kellog$', '$khafji$', '$kellogg$', '$kiewit$', '$kapor$', '$kb$', '$keiaisha$', '$kunz$', '$keene$', '$kayaba$', '$kevlar$', '$kcbot$', '$kaufman$', '$kokusai$', '$keihanshin$', '$kippingerpennsylvania$', '$killearnkpi$', '$kuwaitiraniraq$', '$kindercare$', '$ksz$', '$kittanning$', '$kilo$', '$kicked$', '$kingdom$', '$kohli$', '$kab$', '$klm$', '$kaufmann$', '$kalgoorlie$', '$kinshasa$', '$koza$', '$kodaks$', '$kio$', '$kulyab$', '$karlherbert$', '$kli$', '$killearn$', '$kol$', '$kyc$', '$keyboard$', '$kozas$', '$karsten$', '$krestmark$', '$kinnock$', '$kba$', '$koba$', '$kpst$', '$kuznetsov$', '$kumttok$', '$kph$', '$kerrville$', '$klein$', '$kpa$', '$kaske$', '$kye$', '$kfhgf$', '$kongs$', '$karstadt$', '$kidd$', '$kuroski$', '$ketchum$', '$karbala$', '$kinbara$', '$khor$', '$khashoggis$', '$kreir$', '$kanata$', '$kabivitrum$', '$kaufmangreenspanvolcker$', '$kitamura$', '$keg$', '$kidders$', '$kernel$', '$kearney$', '$keegan$', '$katsunosuke$', '$kum$', '$kell$', '$kuna$', '$kirnan$', '$kelvinator$', '$korean$', '$kleervu$', '$katsuyuki$', '$kwamashu$', '$kalimantan$', '$kunthon$', '$kirin$', '$kahls$', '$kmextv$', '$knbwt$', '$komm$', '$kmart$', '$knitted$', '$kraljevo$', '$kiernan$', '$krowe$', '$kerosene$', '$krummrich$', '$kjeldsen$', '$kelseyhayes$', '$kissack$', '$karl$', '$kanaan$', '$kurunegala$', '$kvaerner$', '$koizumi$', '$kern$', '$kubiak$', '$ktcc$', '$kamal$', '$kayne$', '$kirkland$', '$kirkuk$', '$kate$', '$kotts$', '$kept$', '$knew$', '$kurd$', '$kluwas$', '$kinross$', '$kasler$', '$koncar$', '$kidney$', '$knut$', '$keihin$', '$kaiser$', '$katzka$', '$katopola$', '$kinark$', '$kenya$', '$kabul$', '$kapnick$', '$krutikhin$', '$kaczurba$', '$kncdo$', '$keener$', '$kasgeld$', '$kge$', '$kalbag$', '$kaydon$', '$krn$', '$kang$', '$knifed$', '$ketchums$', '$kenney$', '$keaton$', '$kubt$', '$kma$', '$kensek$', '$kamp$', '$kierans$', '$khaleda$', '$klv$', '$knokke$', '$komatsu$', '$kapnik$', '$kabel$', '$kdnyp$', '$kmt$', '$kane$', '$keboo$', '$keijirou$', '$kamin$', '$killed$', '$kpk$', '$klerk$']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "print(\"results for \\'h*ell\\' are \\n\",wild_find('h*ell', ngram_word_index))\n",
    "print(\"results for \\'j*far\\' are \\n\",wild_find('j*far', ngram_word_index))\n",
    "print(\"results for \\'jaafar\\' are \\n\",wild_find('jaafar', ngram_word_index))\n",
    "\n",
    "print(wild_find('k*', ngram_word_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Soundex algorithm\n",
    "\n",
    "Here I will explore the algorithm of soundex presented in lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def soundex_encode(word):\n",
    "    assert len(word)>0, \"Word should be non empty\"\n",
    "    dd = {'bfpv':'1','cgjkqsxz':'2', 'dt':'3','l':'4','mn':'5','r':'6'}\n",
    "    d = {}\n",
    "    for i in dd.keys():\n",
    "        for j in i:\n",
    "            d[j] = dd[i]\n",
    "    word =  word[0].upper() + \"\".join(map(lambda x: d[x],list(re.sub(r'[aeiouhwy]', '', word[1:]))))\n",
    "    \"\"\"last = '#'   # ask rustam if slide number 39 makes sense to remove consecutive digits\n",
    "    nword = ''\n",
    "    for i in word:\n",
    "        if i == last:\n",
    "            continue\n",
    "        last = i\n",
    "        nword = nword + i\n",
    "    word = nword\"\"\"\n",
    "    while len(word) < 4:\n",
    "        word = word + \"0\"\n",
    "    return word[:4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H655'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "soundex_encode(\"Herman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Levenshtein distance\n",
    "\n",
    "Here we see a pairwise cumputation of Levenshtein distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def edit_distance(first, second):\n",
    "    N, M = len(first), len(second)\n",
    "    dp = [[0 for i in range(M+1) ] for j in range(N+1)]\n",
    "    for i in range(N+1):\n",
    "        dp[i][0] = i\n",
    "    for i in range(M+1):\n",
    "        dp[0][i] = i\n",
    "    for i in range(1,N+1,1):\n",
    "        for j in range(1,M+1,1):\n",
    "            dp[i][j] = min(dp[i-1][j-1] + (1 if first[i-1] != second[j-1] else 0),dp[i-1][j] + 1, dp[i][j-1]+1)\n",
    "    return dp[N][M]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('worrld','world')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFAGHwgY4oPm"
   },
   "source": [
    "# 6. Inverted index\n",
    "You will work with the boolean search model. Construct a dictionary which maps words to the postings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-sYCRN14tEe"
   },
   "outputs": [],
   "source": [
    "def make_index(collection):\n",
    "    inverted_index = {}\n",
    "    for (group,name) in collection:\n",
    "        #print(name)\n",
    "        for word in group:\n",
    "            if word in inverted_index.keys():\n",
    "                inverted_index[word].add(name)\n",
    "            else:\n",
    "                inverted_index[word] = set([name])\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "TlXaMNADNgvi",
    "outputId": "5720806f-aaab-4341-aa84-0efe2cc740a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}\n"
     ]
    }
   ],
   "source": [
    "index = make_index(collection)\n",
    "\n",
    "print(index['$food$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Spelling correction \n",
    "\n",
    "Here i will have a function that given a lexicon will return some possible ways that are close to the original meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_options(token,inverted_word_index,ngram_word_index):\n",
    "    setty = None\n",
    "    grams = to_n_gram(token,2)\n",
    "    for gram in grams:\n",
    "        if not setty:\n",
    "            setty = ngram_word_index[gram]\n",
    "        setty = setty.union(ngram_word_index[gram])\n",
    "\n",
    "    mxi, oword = 1000, ''\n",
    "    arr = []\n",
    "    for word in setty:\n",
    "        edit_dst = edit_distance(token,word)\n",
    "        if edit_dst > 3:\n",
    "            continue\n",
    "        arr.append((edit_dst, word))\n",
    "    arr = sorted(arr)\n",
    "    #print(arr[:4])\n",
    "    return [i[1] for i in arr[:5] if i[0] == arr[0][0]]\n",
    "            \n",
    "def correct_spelling(text, inverted_word_index, ngram_word_index):\n",
    "    tokens = preprocess(text)\n",
    "    print(\"query is \", tokens)\n",
    "    new_text = \"\"\n",
    "    for token in tokens:\n",
    "        \n",
    "        options = get_options(token,inverted_word_index,ngram_word_index)\n",
    "        oword = options[0]\n",
    "        new_text = new_text + \" \" + oword\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is  ['$inaguration$', '$respctful$', '$consititution$', '$jafar$']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' $inauguration$ $respectful$ $constitution$ $jaafar$'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spelling('inaguration respctful consititution jafar',inverted_word_index, ngram_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 8. Logical OR operation in the binary search model\n",
    " \n",
    " \n",
    " Here we can process wild cards as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyIEvWxz6j9n"
   },
   "outputs": [],
   "source": [
    "def search(index, query, collection):\n",
    "    query_copy = preprocess(query, is_query = True)\n",
    "    expression = ''\n",
    "    relevant = None\n",
    "    for word in query_copy:\n",
    "        potential_words = wild_find(word, ngram_word_index)\n",
    "        if word.find('*') ==-1:\n",
    "            potential_words = get_options(\"$\"+word+\"$\",inverted_word_index,ngram_word_index)\n",
    "        expression = expression + \" && (\"+ \" || \".join(potential_words) + \") \"\n",
    "       \n",
    "        if not any(word in index for word in potential_words):\n",
    "            return set()\n",
    "        new_set = set([])\n",
    "        new_set = new_set.union(*[index[w] for w in potential_words]) # Union for multiple documents\n",
    "        \n",
    "        if relevant:\n",
    "            relevant = relevant.intersection(new_set)\n",
    "        else:\n",
    "            relevant = new_set\n",
    "    relevant_documents = [collection[doc_id] for doc_id in relevant]\n",
    "    print(\"Query expression is :: \",expression[3:])\n",
    "    return relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xpr-6e2vKjR5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query expression is ::   ($fancy$ || $fauci$)  && ($word$ || $world$ || $worry$) \n",
      "how many relevant docs:  2\n"
     ]
    }
   ],
   "source": [
    "query = 'fanci worrd' # change for something else if you are searching song lyrics\n",
    "relevant = search(index, query, collection)\n",
    "print(\"how many relevant docs: \",len(relevant))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
